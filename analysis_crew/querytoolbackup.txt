import os
import json
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.tools import tool
from langchain.chains.summarize import load_summarize_chain
from langchain_openai import AzureChatOpenAI
import json
from langchain.tools import tool
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv('.env'))
import json
import os
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
#nltk.download('punkt')
#nltk.download('stopwords')
deployment_name4 = "gpt-4"

llm_gpt4 = AzureChatOpenAI(deployment_name=deployment_name4, model_name=deployment_name4, temperature=0, streaming=True)
path = r'C:\Users\Admin\Desktop\erdcDBFunc\crewAIDocSum\documents'
path_summary =  r'C:\Users\Admin\Desktop\erdcDBFunc\crewAIDocSum\summaries'


class QueryDocumentAnalysis:
    @tool("Query_and_Document_Summary_Analysis")
    def analyze_query_and_summaries(query, summaries_path):
        """
        Analyzes user queries against document summaries to determine if a single or multiple documents are needed to answer the query.
        """
        try:
            with open(summaries_path, 'r') as file:
                document_summaries = json.load(file)
        except Exception as e:
            return {"error": str(e)}

        # Prepare the query and summaries for TF-IDF vectorization
        documents = [query] + [summary['summary'] for summary in document_summaries]
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(documents)
        
        # Calculate cosine similarity between the query and each document summary
        cosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
        
        # Filter out documents that do not meet a minimal similarity threshold
        threshold = 0.1  # This threshold can be adjusted based on your specificity needs
        relevant_documents = []
        for idx, score in enumerate(cosine_similarities):
            if score > threshold:
                relevant_document = document_summaries[idx]
                relevant_documents.append({
                    "title": relevant_document['title'],
                    "path": relevant_document['path'],
                    "similarity_score": score
                })
        
        # Decision making based on the number of relevant documents found
        if len(relevant_documents) == 1:
            return {'need_single_doc': True, 'document': relevant_documents[0]}
        elif len(relevant_documents) > 1:
            return {'need_multiple_docs': True, 'documents': relevant_documents}
        else:
            return {'need_single_doc': False, 'documents': []}






    